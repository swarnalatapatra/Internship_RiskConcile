{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumi\\Anaconda3\\envs\\NLP\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import gensim as gs\n",
    "import scipy as sc\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.tokenize import sent_tokenize as st\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import backend as k\n",
    "k.set_learning_phase(1)\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LSTM,Input,Activation,Add,TimeDistributed,Permute,Flatten,RepeatVector,merge,Lambda,Multiply,Reshape\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for daily load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsetext(dire,category,filename):\n",
    "    with open(\"%s\\\\%s\"%(dire+category,filename),'r',encoding=\"Latin-1\") as readin:\n",
    "#         print(\"file read successfully\")\n",
    "        text=readin.read()\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def load_data(dire,category):\n",
    "    \"\"\"dataname refers to either training, test or validation\"\"\"\n",
    "    for dirs,subdr, files in os.walk(dire+category):\n",
    "        filenames=files\n",
    "    return filenames\n",
    "\n",
    "def cleantext(text):\n",
    "    text=re.sub(r\"what's\",\"what is \",text)\n",
    "    text=re.sub(r\"it's\",\"it is \",text)\n",
    "    text=re.sub(r\"\\'ve\",\" have \",text)\n",
    "    text=re.sub(r\"i'm\",\"i am \",text)\n",
    "    text=re.sub(r\"\\'re\",\" are \",text)\n",
    "    text=re.sub(r\"n't\",\" not \",text)\n",
    "    text=re.sub(r\"\\'d\",\" would \",text)\n",
    "    text=re.sub(r\"\\'s\",\"s\",text)\n",
    "    text=re.sub(r\"\\'ll\",\" will \",text)\n",
    "    text=re.sub(r\"can't\",\" cannot \",text)\n",
    "    text=re.sub(r\" e g \",\" eg \",text)\n",
    "    text=re.sub(r\"e-mail\",\"email\",text)\n",
    "    text=re.sub(r\"9\\\\/11\",\" 911 \",text)\n",
    "    text=re.sub(r\" u.s\",\" american \",text)\n",
    "    text=re.sub(r\" u.n\",\" united nations \",text)\n",
    "    text=re.sub(r\"\\n\",\" \",text)\n",
    "    text=re.sub(r\":\",\" \",text)\n",
    "    text=re.sub(r\"-\",\" \",text)\n",
    "    text=re.sub(r\"\\_\",\" \",text)\n",
    "    text=re.sub(r\"\\d+\",\" \",text)\n",
    "    text=re.sub(r\"[$#@%&*!~?%{}()]\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def printArticlesum(k):\n",
    "    print(\"---------------------original sentence-----------------------\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(data[\"articles\"][k])\n",
    "    print(\"----------------------Summary sentence-----------------------\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(data[\"summaries\"][k])\n",
    "    return 0\n",
    "\n",
    "\n",
    "def announcedone():\n",
    "    duration=2000\n",
    "    freq=440\n",
    "    ws.Beep(freq,duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCorpus(t):\n",
    "    corpus = []\n",
    "    all_sent = []\n",
    "    for k in t:\n",
    "        for p in t[k]:\n",
    "            corpus.append(st(p))\n",
    "    for sent in range(len(corpus)):\n",
    "        for k in corpus[sent]:\n",
    "            all_sent.append(k)\n",
    "    for m in range(len(all_sent)):\n",
    "        all_sent[m] = wt(all_sent[m])\n",
    "    \n",
    "    all_words=[]\n",
    "    for sent in all_sent:\n",
    "        hold=[]\n",
    "        for word in sent:\n",
    "            hold.append(word.lower())\n",
    "        all_words.append(hold)\n",
    "    return all_words\n",
    "\n",
    "def word2vecmodel(corpus):\n",
    "    emb_size = emb_size_all\n",
    "    model_type={\"skip_gram\":1,\"CBOW\":0}\n",
    "    window=10\n",
    "    workers=4\n",
    "    min_count=4\n",
    "    batch_words=20\n",
    "    epochs=25\n",
    "    #include bigrams\n",
    "    #bigramer = gs.models.Phrases(corpus)\n",
    "\n",
    "    model=gs.models.Word2Vec(corpus,size=emb_size,sg=model_type[\"skip_gram\"],\n",
    "                             compute_loss=True,window=window,min_count=min_count,workers=workers,\n",
    "                             batch_words=batch_words)\n",
    "        \n",
    "    model.train(corpus,total_examples=len(corpus),epochs=epochs)\n",
    "#     model.save(\"%sWord2vec\"%modelLocation)\n",
    "    model.save(ROOT + '/TrainedModels/' + str(epochs) +'_epochs_word2vec_model.h5')\n",
    "\n",
    "    print('\\007')\n",
    "    return model\n",
    "\n",
    "def summonehot(corpus):\n",
    "    allwords=[]\n",
    "    annotated={}\n",
    "    for sent in corpus:\n",
    "        for word in wt(sent):\n",
    "            allwords.append(word.lower())\n",
    "    print(len(set(allwords)), \"unique characters in corpus\")\n",
    "    #maxcorp=int(input(\"Enter desired number of vocabulary: \"))\n",
    "    maxcorp=int(len(set(allwords))/1.1)\n",
    "    wordcount = Counter(allwords).most_common(maxcorp)\n",
    "    allwords=[]\n",
    "    \n",
    "    for p in wordcount:\n",
    "        allwords.append(p[0])  \n",
    "        \n",
    "    allwords=list(set(allwords))\n",
    "    \n",
    "    print(len(allwords), \"unique characters in corpus after max corpus cut\")\n",
    "    #integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(allwords)\n",
    "    #one hot\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    #make look up dict\n",
    "    for k in range(len(onehot_encoded)): \n",
    "        inverted = cleantext(label_encoder.inverse_transform([argmax(onehot_encoded[k, :])])[0]).strip()\n",
    "        annotated[inverted]=onehot_encoded[k]\n",
    "    return label_encoder,onehot_encoded,annotated\n",
    "\n",
    "def wordvecmatrix(model,data):\n",
    "    IO_data={\"article\":[],\"summaries\":[]}\n",
    "    i=1\n",
    "    for k in range(len(data[\"articles\"])):\n",
    "        art=[]\n",
    "        summ=[]\n",
    "        for word in wt(data[\"articles\"][k].lower()):\n",
    "            try:\n",
    "                art.append(model.wv.word_vec(word))\n",
    "            except Exception as e:\n",
    "                e\n",
    "#                 print(e)\n",
    "\n",
    "        for word in wt(data[\"summaries\"][k].lower()):\n",
    "            try:\n",
    "#                 summ.append(onehot[word])\n",
    "                summ.append(model.wv.word_vec(word))\n",
    "            except Exception as e:\n",
    "                e\n",
    "#                 print(e)\n",
    "        \n",
    "        IO_data[\"article\"].append(art) \n",
    "        IO_data[\"summaries\"].append(summ)\n",
    "        if i%100==0:\n",
    "            print(\"progress: \" + str(((i*100)/len(data[\"articles\"]))))\n",
    "        i+=1\n",
    "    #announcedone()\n",
    "    print('\\007')\n",
    "    return IO_data\n",
    "\n",
    "def cutoffSequences(data,artLen,sumlen):\n",
    "    data2={\"article\":[],\"summaries\":[]}\n",
    "    for k in range(len(data[\"article\"])):\n",
    "        if len(data[\"article\"][k])<artLen or len(data[\"summaries\"][k])<sumlen:\n",
    "             #data[\"article\"]=np.delete(data[\"article\"],k,0)\n",
    "             #data[\"article\"]=np.delete(data[\"summaries\"],k,0)\n",
    "             pass\n",
    "        else:\n",
    "            data2[\"article\"].append(data[\"article\"][k][:artLen])\n",
    "            data2[\"summaries\"].append(data[\"summaries\"][k][:sumlen])\n",
    "    return data2\n",
    "\n",
    "\n",
    "def max_len(data):\n",
    "    lenk=[]\n",
    "    for k in data:\n",
    "        lenk.append(len(k))\n",
    "    print(\"The minimum length is: \",min(lenk))\n",
    "    print(\"The average length is: \",np.average(lenk))\n",
    "    print(\"The max length is: \",max(lenk))\n",
    "    return min(lenk),max(lenk)\n",
    "\n",
    "\"\"\"reshape vectres for Gensim\"\"\"\n",
    "def reshape(vec):\n",
    "    return np.reshape(vec,(1,emb_size_all))\n",
    "\n",
    "def addones(seq):\n",
    "    return np.insert(seq, [0], [[0],], axis = 0)\n",
    "\n",
    "def endseq(seq):\n",
    "    pp=len(seq)\n",
    "    return np.insert(seq, [pp], [[1],], axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder(data):\n",
    "    print('Encoder_Decoder LSTM...')\n",
    "   \n",
    "    \"\"\"__encoder___\"\"\"\n",
    "    encoder_inputs = Input(shape=en_shape)\n",
    "    \n",
    "    encoder_LSTM = LSTM(hidden_units, dropout_U = 0.2, dropout_W = 0.2 ,return_state=True)\n",
    "    encoder_LSTM_rev=LSTM(hidden_units,return_state=True,go_backwards=True)\n",
    "    \n",
    "    #merger=Add()[encoder_LSTM(encoder_inputs), encoder_LSTM_rev(encoder_inputs)]\n",
    "    encoder_outputsR, state_hR, state_cR = encoder_LSTM_rev(encoder_inputs)\n",
    "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_inputs)\n",
    "    \n",
    "    state_hfinal=Add()([state_h,state_hR])\n",
    "    state_cfinal=Add()([state_c,state_cR])\n",
    "    \n",
    "    encoder_states = [state_hfinal,state_cfinal]\n",
    "    \n",
    "    \"\"\"____decoder___\"\"\"\n",
    "    decoder_inputs = Input(shape=(None,de_shape[1]))\n",
    "    decoder_LSTM = LSTM(hidden_units,return_sequences=True,return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_LSTM(decoder_inputs,initial_state=encoder_states) \n",
    "    decoder_dense = Dense(de_shape[1],activation='linear')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
    "    #plot_model(model, to_file=modelLocation+'model.png', show_shapes=True)\n",
    "    rmsprop = RMSprop(lr=learning_rate,clipnorm=clip_norm)\n",
    "    \n",
    "    model.compile(loss='mse',optimizer=rmsprop,metrics=['accuracy'])\n",
    "\n",
    "    x_train,x_test,y_train,y_test=tts(data[\"article\"],data[\"summaries\"],test_size=0.20)\n",
    "    history = model.fit(x=[x_train,y_train],\n",
    "              y=y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=([x_test,y_test], y_test))\n",
    "    \n",
    "    \"\"\"_________________inference mode__________________\"\"\"\n",
    "    encoder_model_inf = Model(encoder_inputs,encoder_states)\n",
    "    \n",
    "    decoder_state_input_H = Input(shape=(hidden_units,))\n",
    "    decoder_state_input_C = Input(shape=(hidden_units,)) \n",
    "    decoder_state_inputs = [decoder_state_input_H, decoder_state_input_C]\n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_LSTM(decoder_inputs,\n",
    "                                                                     initial_state=decoder_state_inputs)\n",
    "    decoder_states = [decoder_state_h, decoder_state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model_inf= Model([decoder_inputs]+decoder_state_inputs,\n",
    "                         [decoder_outputs]+decoder_states)\n",
    "    \n",
    "    \n",
    "    #plot_model(encoder_model_inf, to_file='encoder_model.png', show_shapes=True)\n",
    "    #plot_model(decoder_model_inf, to_file='decoder_model.png', show_shapes=True)\n",
    "    scores = model.evaluate([x_test,y_test],y_test, verbose=0)\n",
    "    \n",
    "    \n",
    "    print('LSTM test scores:', scores)\n",
    "    #announcedone()\n",
    "    print('\\007')\n",
    "    print(model.summary())\n",
    "    return model,encoder_model_inf,decoder_model_inf,history\n",
    "\n",
    "\"\"\"___pred____\"\"\"\n",
    "def comparePred(index):\n",
    "    pred=trained_model.predict([np.reshape(train_data[\"article\"][index],(1,en_shape[0],emb_size_all)),np.reshape(train_data[\"summaries\"][index],(1,de_shape[0],emb_size_all))])\n",
    "    return pred\n",
    "\n",
    "\n",
    "\"\"\"____generate summary from vectors and remove padding words___\"\"\"\n",
    "def generateText(SentOfVecs):\n",
    "    SentOfVecs=np.reshape(SentOfVecs,de_shape)\n",
    "    kk=\"\"\n",
    "    for k in SentOfVecs:\n",
    "        kk = kk + label_encoder.inverse_transform([argmax(k)])[0].strip()+\" \"\n",
    "#         kk=kk+((getWord(k)[0]+\" \") if getWord(k)[1]>0.2 else \"\")\n",
    "    return kk\n",
    "\n",
    "\"\"\"___generate summary vectors___\"\"\"\n",
    "\n",
    "def summarize(article):\n",
    "    stop_pred = False\n",
    "    article =  np.reshape(article,(1,en_shape[0],en_shape[1]))\n",
    "    \n",
    "    #get initial h and c values from encoder\n",
    "    init_state_val = encoder.predict(article)\n",
    "    target_seq = np.zeros((1,1,emb_size_all))\n",
    "    \n",
    "    generated_summary=[]\n",
    "    while not stop_pred:\n",
    "        decoder_out,decoder_h,decoder_c= decoder.predict(x=[target_seq]+init_state_val)\n",
    "        generated_summary.append(decoder_out)\n",
    "        init_state_val= [decoder_h,decoder_c]\n",
    "        #get most similar word and put in line to be input in next timestep\n",
    "        #target_seq=np.reshape(model.wv[getWord(decoder_out)[0]],(1,1,emb_size_all))\n",
    "        target_seq=np.reshape(decoder_out,(1,1,emb_size_all))\n",
    "        if len(generated_summary)== de_shape[0]:\n",
    "            stop_pred=True\n",
    "            break\n",
    "    return generated_summary\n",
    "\n",
    "\"\"\"__________________Plot training curves_______________\"\"\"\n",
    "\n",
    "def plot_training(history):\n",
    "    print(history.history.keys())\n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    # \"Loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_summ(article):\n",
    "    ref=''\n",
    "    for k in wt(data['summaries'][article])[:20]:\n",
    "        ref=ref+' '+k\n",
    "    gen_sum = generateText(summarize(new_train_data[\"article\"][article]))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Original summary\")\n",
    "    print(ref)\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"Generated summary\")\n",
    "    print(gen_sum)\n",
    "    print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob  #Sentiment Analysis\n",
    "def fill_sentiments(data):\n",
    "    data['articles'] = 0.0\n",
    "    data['summaries'] = 0.0\n",
    "    for arti,summ in data.iterrows():\n",
    "        cleaned_tweet = row['text']\n",
    "        s_analysis = TextBlob(cleaned_tweet)\n",
    "        tweet_df.at[index,'tweet_sentiment_polarity'] = s_analysis.sentiment.polarity\n",
    "        tweet_df.at[index,'tweet_sentiment_subjectivity'] = s_analysis.sentiment.subjectivity\n",
    "    return tweet_df\n",
    "\n",
    "'''Word count of text: To divide the AFINN score with no. of words.'''\n",
    "def split(delimiters, string, maxsplit=0):\n",
    "    import re\n",
    "    regexPattern = '|'.join(map(re.escape, delimiters))\n",
    "    return re.split(regexPattern, string, maxsplit)\n",
    "\n",
    "def word_count(string):\n",
    "    '''Split words in the text delimited by below'''\n",
    "    delimiters = \".\",\" \"\n",
    "    '''Calculate the number of words in a string'''\n",
    "    return len(split(delimiters, string, maxsplit=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below is commented out as we are not using this currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load from file into dictionaries\n",
    "# import pickle\n",
    "# cnn_stories = pickle.load(open(ROOT +'/cnn_dataset.pkl', 'rb'))\n",
    "# dm_stories = pickle.load(open(ROOT +'/dm_dataset.pkl', 'rb'))\n",
    "# cnn_slen = len(cnn_stories)\n",
    "# dm_slen = len(dm_stories)\n",
    "# print(cnn_slen,\"\\n\",dm_slen)\n",
    "\n",
    "# cnn_stories[0]['highlights']\n",
    "\n",
    "# dm_stories[0]['highlights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Using CountVectorizer to extract sentence vectors and unique words/features\n",
    "# \"\"\"\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# X = [] # X[i].toarray().shape gives (no. of sentences in article/summary i, no. of unique words in all sentences )\n",
    "# f = [] # f[i] gives a list of all unique words in aticle/summary i (doesn't include stop words)\n",
    "\n",
    "# for i in range(len(corpus)):\n",
    "#     if (len(corpus[i])>0):\n",
    "#         vectorizer = CountVectorizer()\n",
    "#         X.append(vectorizer.fit_transform(corpus[i]))\n",
    "#         f.append(vectorizer.get_feature_names())\n",
    "#     else:\n",
    "#         print(\"Encountered empty line - Skipped at\", i)\n",
    "\n",
    "# # # To obtain the no. of unique words\n",
    "# # label_encoder,onehot_encoded,onehot=summonehot(data[\"summaries\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for republic.ipynb summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    # select a seed text\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # the seed text must be encoded to integers using the same tokenizer that we used when training the model.\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
